#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è±†ç“£ç”µå½±/å›¾ä¹¦è¯„è®ºåˆ†æç³»ç»Ÿä¸»ç¨‹åº
ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0.0
"""

import os
import sys
import argparse
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.crawler import DoubanCrawler
from src.text_analyzer import TextAnalyzer
from src.wordcloud_generator import WordCloudGenerator
from src.sentiment_analyzer import SentimentAnalyzer
from src.knowledge_graph import KnowledgeGraphBuilder
from config.config import DATA_DIR, OUTPUT_DIR


def ensure_directories():
    """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    directories = [
        DATA_DIR,
        OUTPUT_DIR,
        os.path.join(OUTPUT_DIR, 'wordclouds'),
        os.path.join(OUTPUT_DIR, 'charts'),
        os.path.join(OUTPUT_DIR, 'graphs'),
        os.path.join(os.path.dirname(__file__), 'logs')
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"åˆ›å»ºç›®å½•: {directory}")


def collect_data(content_type, content_name, max_comments):
    """
    æ”¶é›†æ•°æ®
    
    Args:
        content_type (str): å†…å®¹ç±»å‹ ('movie' æˆ– 'book')
        content_name (str): å†…å®¹åç§°
        max_comments (int): æœ€å¤§è¯„è®ºæ•°é‡
        
    Returns:
        str: ä¿å­˜çš„æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    print(f"\n{'='*50}")
    print(f"å¼€å§‹æ”¶é›†{content_type}è¯„è®ºæ•°æ®")
    print(f"å†…å®¹åç§°: {content_name}")
    print(f"æœ€å¤§è¯„è®ºæ•°: {max_comments}")
    print(f"{'='*50}")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = DoubanCrawler(content_type=content_type, max_comments=max_comments)
    
    try:
        # æ”¶é›†è¯„è®º
        comments = crawler.crawl_comments(content_name)
        
        if not comments:
            print("æœªæ”¶é›†åˆ°ä»»ä½•è¯„è®ºæ•°æ®")
            return None
        
        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{content_name}_{content_type}_{timestamp}.txt"
        filepath = crawler.save_comments(filename)
        
        if filepath:
            print(f"\næ•°æ®æ”¶é›†å®Œæˆï¼")
            print(f"å…±æ”¶é›†åˆ° {len(comments)} æ¡è¯„è®º")
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            
            return filepath
        else:
            print("æ•°æ®ä¿å­˜å¤±è´¥")
            return None
        
    except Exception as e:
        print(f"æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return None
    finally:
        if crawler.driver:
            crawler.driver.quit()


def analyze_text(filepath):
    """
    è¿›è¡Œæ–‡æœ¬åˆ†æ
    
    Args:
        filepath (str): æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    print(f"\n{'='*50}")
    print("å¼€å§‹æ–‡æœ¬åˆ†æ")
    print(f"æ•°æ®æ–‡ä»¶: {filepath}")
    print(f"{'='*50}")
    
    try:
        # åˆ›å»ºæ–‡æœ¬åˆ†æå™¨
        analyzer = TextAnalyzer()
        
        # åŠ è½½æ•°æ®
        analyzer.load_data_from_file(filepath)
        
        # æ‰§è¡Œåˆ†æ
        analyzer.analyze_word_frequency()
        keywords = analyzer.extract_keywords()
        
        # ç”Ÿæˆå›¾è¡¨
        charts_dir = os.path.join(OUTPUT_DIR, 'charts')
        os.makedirs(charts_dir, exist_ok=True)
        
        analyzer.plot_word_frequency(
            save_path=os.path.join(charts_dir, 'word_frequency.png')
        )
        analyzer.plot_time_word_trend(
            save_path=os.path.join(charts_dir, 'time_word_trend.png')
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        analyzer.save_analysis_results(OUTPUT_DIR)
        
        # è·å–åˆ†ææŠ¥å‘Š
        results = analyzer.get_analysis_report()
        
        print(f"\næ–‡æœ¬åˆ†æå®Œæˆï¼")
        print(f"æ€»è¯æ•°: {results.get('basic_stats', {}).get('total_words', 0)}")
        print(f"å»é‡è¯æ•°: {results.get('basic_stats', {}).get('unique_words', 0)}")
        print(f"é«˜é¢‘è¯æ•°: {len(results.get('top_words', []))}")
        
        return results
        
    except Exception as e:
        print(f"æ–‡æœ¬åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_wordcloud(text_data, output_name):
    """
    ç”Ÿæˆè¯äº‘å›¾
    
    Args:
        text_data (dict): è¯é¢‘æ•°æ®å­—å…¸
        output_name (str): è¾“å‡ºæ–‡ä»¶åå‰ç¼€
    """
    print(f"\n{'='*50}")
    print("å¼€å§‹ç”Ÿæˆè¯äº‘å›¾")
    print(f"{'='*50}")
    
    try:
        # åˆ›å»ºè¯äº‘ç”Ÿæˆå™¨
        generator = WordCloudGenerator()
        
        wordclouds_dir = os.path.join(OUTPUT_DIR, 'wordclouds')
        os.makedirs(wordclouds_dir, exist_ok=True)
        
        # ç”Ÿæˆå¤šç§ç±»å‹çš„è¯äº‘
        wordcloud_files = []
        
        # ä»æ–‡æœ¬æ•°æ®ä¸­æå–è¯é¢‘
        if isinstance(text_data, dict) and 'top_words' in text_data:
            word_freq_dict = dict(text_data['top_words'])
        elif isinstance(text_data, dict):
            word_freq_dict = text_data
        else:
            # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œåˆ›å»ºé»˜è®¤è¯é¢‘
            word_freq_dict = {
                'ç²¾å½©': 100, 'æ„ŸåŠ¨': 90, 'å‰§æƒ…': 85, 'æ¼”æŠ€': 80, 'å¯¼æ¼”': 75,
                'éŸ³ä¹': 70, 'ç‰¹æ•ˆ': 65, 'å°è¯': 60, 'æ‘„å½±': 55, 'èŠ‚å¥': 50
            }
        
        # 1. åŸºç¡€è¯äº‘
        basic_wordcloud = generator.create_wordcloud(word_freq_dict)
        basic_file = os.path.join(wordclouds_dir, f'{output_name}_basic.png')
        if generator.save_wordcloud(basic_file):
            wordcloud_files.append(basic_file)
        
        # 2. å¿ƒå½¢è¯äº‘
        heart_wordcloud = generator.create_shaped_wordcloud(word_freq_dict, 'heart')
        heart_file = os.path.join(wordclouds_dir, f'{output_name}_heart.png')
        if generator.save_wordcloud(heart_file):
            wordcloud_files.append(heart_file)
        
        # 3. æ˜Ÿå½¢è¯äº‘
        star_wordcloud = generator.create_shaped_wordcloud(word_freq_dict, 'star')
        star_file = os.path.join(wordclouds_dir, f'{output_name}_star.png')
        if generator.save_wordcloud(star_file):
            wordcloud_files.append(star_file)
        
        print(f"\nè¯äº‘ç”Ÿæˆå®Œæˆï¼")
        print(f"ç”Ÿæˆäº† {len(wordcloud_files)} ä¸ªè¯äº‘æ–‡ä»¶:")
        for file in wordcloud_files:
            print(f"  - {file}")
        
        return wordcloud_files
        
    except Exception as e:
        print(f"è¯äº‘ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


def analyze_sentiment(filepath):
    """
    è¿›è¡Œæƒ…æ„Ÿåˆ†æ
    
    Args:
        filepath (str): æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: æƒ…æ„Ÿåˆ†æç»“æœ
    """
    print(f"\n{'='*50}")
    print("å¼€å§‹æƒ…æ„Ÿåˆ†æ")
    print(f"{'='*50}")
    
    try:
        # åˆ›å»ºæƒ…æ„Ÿåˆ†æå™¨
        analyzer = SentimentAnalyzer()
        
        # åŠ è½½æ•°æ®
        analyzer.load_data_from_file(filepath)
        
        # æ‰§è¡Œåˆ†æ
        analyzer.analyze_sentiment()
        
        # ç”Ÿæˆå›¾è¡¨
        charts_dir = os.path.join(OUTPUT_DIR, 'charts')
        os.makedirs(charts_dir, exist_ok=True)
        
        analyzer.plot_sentiment_distribution(
            save_path=os.path.join(charts_dir, 'sentiment_distribution.png')
        )
        analyzer.plot_time_sentiment_trend(
            save_path=os.path.join(charts_dir, 'sentiment_time_trend.png')
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        analyzer.save_sentiment_results(OUTPUT_DIR)
        
        # è·å–æç«¯è¯„è®º
        analyzer.get_extreme_comments()
        
        # è·å–åˆ†ææ‘˜è¦
        results = analyzer.get_sentiment_summary()
        
        print(f"\næƒ…æ„Ÿåˆ†æå®Œæˆï¼")
        print(f"å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {results.get('avg_sentiment_score', 0):.3f}")
        print(f"ç§¯æè¯„è®º: {results.get('positive_count', 0)} æ¡")
        print(f"æ¶ˆæè¯„è®º: {results.get('negative_count', 0)} æ¡")
        print(f"ä¸­æ€§è¯„è®º: {results.get('neutral_count', 0)} æ¡")
        
        return results
        
    except Exception as e:
        print(f"æƒ…æ„Ÿåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def build_knowledge_graph(filepath, output_name):
    """
    æ„å»ºçŸ¥è¯†å›¾è°±
    
    Args:
        filepath (str): æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        output_name (str): è¾“å‡ºæ–‡ä»¶åå‰ç¼€
        
    Returns:
        list: ç”Ÿæˆçš„å›¾è°±æ–‡ä»¶åˆ—è¡¨
    """
    print(f"\n{'='*50}")
    print("å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±")
    print(f"{'='*50}")
    
    try:
        # åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨
        builder = KnowledgeGraphBuilder()
        
        # åŠ è½½æ•°æ®
        builder.load_data_from_file(filepath)
        
        # æå–å®ä½“
        builder.extract_entities()
        
        # æ„å»ºå…³ç³»
        builder.build_relations()
        
        # åˆ›å»ºå›¾è°±
        builder.create_graph()
        
        # ç”Ÿæˆå¯è§†åŒ–
        graphs_dir = os.path.join(OUTPUT_DIR, 'graphs')
        os.makedirs(graphs_dir, exist_ok=True)
        
        graph_files = []
        
        # é™æ€å›¾è°±
        static_file = os.path.join(graphs_dir, f'{output_name}_knowledge_graph.png')
        builder.visualize_graph(save_path=static_file)
        graph_files.append(static_file)
        
        # äº¤äº’å¼å›¾è°±
        interactive_file = os.path.join(graphs_dir, f'{output_name}_interactive_graph.html')
        builder.create_interactive_graph(save_path=interactive_file)
        graph_files.append(interactive_file)
        
        # ä¿å­˜å›¾è°±æ•°æ®
        builder.save_graph_data(OUTPUT_DIR)
        
        print(f"\nçŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        if graph_files:
            print(f"ç”Ÿæˆäº† {len(graph_files)} ä¸ªå›¾è°±æ–‡ä»¶:")
            for file in graph_files:
                print(f"  - {file}")
        
        return graph_files
        
    except Exception as e:
        print(f"çŸ¥è¯†å›¾è°±æ„å»ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


def generate_report(content_name, content_type, analysis_results):
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    
    Args:
        content_name (str): å†…å®¹åç§°
        content_type (str): å†…å®¹ç±»å‹
        analysis_results (dict): æ‰€æœ‰åˆ†æç»“æœ
    """
    print(f"\n{'='*50}")
    print("ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    print(f"{'='*50}")
    
    try:
        report_path = os.path.join(OUTPUT_DIR, 'analysis_report.html')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è±†ç“£{content_type}è¯„è®ºåˆ†ææŠ¥å‘Š - {content_name}</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 10px; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .stats {{ display: flex; justify-content: space-around; margin: 20px 0; }}
        .stat-item {{ text-align: center; padding: 10px; background: #e8f4fd; border-radius: 5px; }}
        .top-words {{ display: flex; flex-wrap: wrap; gap: 10px; }}
        .word-item {{ background: #f8f9fa; padding: 5px 10px; border-radius: 15px; font-size: 14px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>è±†ç“£{content_type}è¯„è®ºåˆ†ææŠ¥å‘Š</h1>
        <h2>ã€Š{content_name}ã€‹</h2>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="section">
        <h3>ğŸ“Š æ•°æ®æ¦‚è§ˆ</h3>
        <div class="stats">
            <div class="stat-item">
                <h4>æ€»è¯„è®ºæ•°</h4>
                <p>{analysis_results.get('text_analysis', {}).get('total_comments', 0)}</p>
            </div>
            <div class="stat-item">
                <h4>æ€»è¯æ•°</h4>
                <p>{analysis_results.get('text_analysis', {}).get('total_words', 0)}</p>
            </div>
            <div class="stat-item">
                <h4>å»é‡è¯æ•°</h4>
                <p>{analysis_results.get('text_analysis', {}).get('unique_words', 0)}</p>
            </div>
            <div class="stat-item">
                <h4>å¹³å‡æƒ…æ„Ÿå¾—åˆ†</h4>
                <p>{analysis_results.get('sentiment_analysis', {}).get('avg_sentiment_score', 0):.3f}</p>
            </div>
        </div>
    </div>
    
    <div class="section">
        <h3>ğŸ“ æƒ…æ„Ÿåˆ†æç»“æœ</h3>
        <div class="stats">
            <div class="stat-item">
                <h4>ç§¯æè¯„è®º</h4>
                <p>{analysis_results.get('sentiment_analysis', {}).get('positive_count', 0)} æ¡</p>
            </div>
            <div class="stat-item">
                <h4>ä¸­æ€§è¯„è®º</h4>
                <p>{analysis_results.get('sentiment_analysis', {}).get('neutral_count', 0)} æ¡</p>
            </div>
            <div class="stat-item">
                <h4>æ¶ˆæè¯„è®º</h4>
                <p>{analysis_results.get('sentiment_analysis', {}).get('negative_count', 0)} æ¡</p>
            </div>
        </div>
    </div>
    
    <div class="section">
        <h3>ğŸ”¤ é«˜é¢‘è¯æ±‡</h3>
        <div class="top-words">
""")
            
            # æ·»åŠ é«˜é¢‘è¯æ±‡
            top_words = analysis_results.get('text_analysis', {}).get('top_words', [])
            for word, freq in top_words[:50]:  # æ˜¾ç¤ºå‰50ä¸ªé«˜é¢‘è¯
                f.write(f'            <div class="word-item">{word} ({freq})</div>\n')
            
            f.write("""
        </div>
    </div>
    
    <div class="section">
        <h3>ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨</h3>
        <p>è¯äº‘å›¾å’Œåˆ†æå›¾è¡¨å·²ç”Ÿæˆåˆ° output ç›®å½•ä¸­ï¼Œè¯·æŸ¥çœ‹ç›¸å…³æ–‡ä»¶ã€‚</p>
    </div>
    
    <div class="section">
        <h3>ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±</h3>
        <p>çŸ¥è¯†å›¾è°±æ–‡ä»¶å·²ç”Ÿæˆåˆ° output/graphs ç›®å½•ä¸­ï¼Œè¯·ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼å›¾è°±ã€‚</p>
    </div>
    
</body>
</html>
""")
        
        print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path
        
    except Exception as e:
        print(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°é”™è¯¯: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è±†ç“£ç”µå½±/å›¾ä¹¦è¯„è®ºåˆ†æç³»ç»Ÿ')
    parser.add_argument('--type', choices=['movie', 'book'], default='movie', 
                       help='åˆ†æç±»å‹: movie(ç”µå½±) æˆ– book(å›¾ä¹¦)')
    parser.add_argument('--name', type=str, help='ç”µå½±æˆ–å›¾ä¹¦åç§°')
    parser.add_argument('--max_comments', type=int, default=1000, 
                       help='æœ€å¤§æ”¶é›†è¯„è®ºæ•°é‡')
    parser.add_argument('--analyze_only', action='store_true', 
                       help='ä»…è¿›è¡Œæ–‡æœ¬åˆ†æï¼Œä¸æ”¶é›†æ•°æ®')
    parser.add_argument('--file', type=str, 
                       help='è¦åˆ†æçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_directories()
    
    print("ğŸ¬ è±†ç“£ç”µå½±/å›¾ä¹¦è¯„è®ºåˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # å¦‚æœæ˜¯ä»…åˆ†ææ¨¡å¼
    if args.analyze_only:
        if not args.file:
            print("é”™è¯¯: åˆ†ææ¨¡å¼éœ€è¦æŒ‡å®šæ–‡ä»¶è·¯å¾„ (--file)")
            return
        
        if not os.path.exists(args.file):
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {args.file}")
            return
        
        filepath = args.file
        content_name = os.path.splitext(os.path.basename(filepath))[0]
        
    else:
        # å®Œæ•´æµç¨‹ï¼šæ”¶é›†æ•°æ®
        if not args.name:
            # äº¤äº’å¼è¾“å…¥
            content_type = input("è¯·é€‰æ‹©åˆ†æç±»å‹ (movie/book): ").strip().lower()
            if content_type not in ['movie', 'book']:
                content_type = 'movie'
            
            content_name = input(f"è¯·è¾“å…¥{'ç”µå½±' if content_type == 'movie' else 'å›¾ä¹¦'}åç§°: ").strip()
            if not content_name:
                print("é”™è¯¯: å¿…é¡»æä¾›å†…å®¹åç§°")
                return
        else:
            content_type = args.type
            content_name = args.name
        
        # æ”¶é›†æ•°æ®
        filepath = collect_data(content_type, content_name, args.max_comments)
        if not filepath:
            print("æ•°æ®æ”¶é›†å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
    
    # æ‰§è¡Œåˆ†ææµç¨‹
    analysis_results = {}
    
    # 1. æ–‡æœ¬åˆ†æ
    text_results = analyze_text(filepath)
    if text_results:
        analysis_results['text_analysis'] = text_results
    
    # 2. ç”Ÿæˆè¯äº‘
    if text_results:
        wordcloud_files = generate_wordcloud(text_results, content_name)
        analysis_results['wordcloud_files'] = wordcloud_files
    
    # 3. æƒ…æ„Ÿåˆ†æ
    sentiment_results = analyze_sentiment(filepath)
    if sentiment_results:
        analysis_results['sentiment_analysis'] = sentiment_results
    
    # 4. æ„å»ºçŸ¥è¯†å›¾è°±
    graph_files = build_knowledge_graph(filepath, content_name)
    if graph_files:
        analysis_results['graph_files'] = graph_files
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    report_file = generate_report(content_name, args.type, analysis_results)
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print(f"{'='*60}")
    
    if analysis_results:
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        if 'wordcloud_files' in analysis_results:
            for file in analysis_results['wordcloud_files']:
                print(f"  ğŸ“Š è¯äº‘å›¾: {file}")
        
        if 'graph_files' in analysis_results:
            for file in analysis_results['graph_files']:
                print(f"  ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±: {file}")
        
        if report_file:
            print(f"  ğŸ“‹ åˆ†ææŠ¥å‘Š: {report_file}")
        
        print(f"\nğŸ’¡ æç¤º: æ‰€æœ‰ç»“æœæ–‡ä»¶éƒ½ä¿å­˜åœ¨ {OUTPUT_DIR} ç›®å½•ä¸­")
        print("è¯·ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹è¯¦ç»†ç»“æœï¼")
    
    else:
        print("æœªèƒ½ç”Ÿæˆåˆ†æç»“æœï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")


if __name__ == '__main__':
    main() 